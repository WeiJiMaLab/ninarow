{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe18e5e4-1afd-4949-b2d1-850f76d205e5",
   "metadata": {},
   "source": [
    "# Customizing Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eafb54-e8c9-4245-a780-7e5a2d76df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your other imports here ...\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: replace with your path/to/ninarow\n",
    "ninarowdir = os.path.dirname(os.getcwd())\n",
    "modelfitdir = ninarowdir + \"/model_fitting/\"\n",
    "# os.listdir(modelfitdir)\n",
    "\n",
    "# sets the import path to the model-fitting directory\n",
    "sys.path.insert(0, modelfitdir)\n",
    "from parsers import *\n",
    "from model_fit import *\n",
    "from utils import *\n",
    "import model_fit\n",
    "from tqdm import tqdm\n",
    "from tree_search import *\n",
    "\n",
    "# WARNING: %load_ext autoreload and %autoreload 2 may interfere with \n",
    "# the Multi-threading processes!\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6157b8-7034-436d-b9bd-955041d96492",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341377a9-51b4-44a6-b96d-918d52120dcb",
   "metadata": {},
   "source": [
    "### File Formatting\n",
    "The data columns should be ordered: \n",
    "\n",
    "    - black_pieces (binary), \n",
    "    - white_pieces (binary), \n",
    "    - player_color (Black/White), \n",
    "    - move (binary), \n",
    "    - response time (not used in fitting), \n",
    "    - [group_id] (optional), \n",
    "    - participant_id\n",
    "\n",
    "for more info, see `parsers.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd44263-aa78-4d10-bd90-c418a692eb05",
   "metadata": {},
   "source": [
    "# Fitting the Model to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "output_path = \"../data/out\"\n",
    "n_splits = 5\n",
    "fold_number = 1\n",
    "threads = 1\n",
    "random_sample = False\n",
    "verbose = True\n",
    "\n",
    "print(f\"Building output directory at {output_path}\")\n",
    "os.makedirs(output_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ea3ac-c9f7-4643-b3e2-8d5677b40b99",
   "metadata": {},
   "source": [
    "## Comparing Original Code with Custom Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f73bed-2f4f-434b-bbaa-58d756a9ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Value, set_start_method\n",
    "from prodict import Prodict\n",
    "\n",
    "# first, we have to check to see if all the splits are there ...\n",
    "assert np.all([f\"{i + 1}.csv\" in os.listdir(data_path) for i in range(n_splits)])\n",
    "print(\"Detected splits in this directory. Loading splits ...\")\n",
    "\n",
    "# then we read them in\n",
    "splits = [pd.read_csv(f\"{data_path}/{i + 1}.csv\") for i in range(n_splits)]\n",
    "\n",
    "# we convert every row of our CSV to a \"CSVMove object\" using df_to_CSVMove - we do so for all the splits\n",
    "# CSVMove is a class that is defined in the parsers.py file \n",
    "fold_data = [[csvmove for csvmove in df_to_CSVMove(split, warn = False)] for split in splits]\n",
    "\n",
    "random.seed(10)\n",
    "model_fit.initialize_thread_pool(1, manual_seed = 10)\n",
    "\n",
    "args = Prodict()\n",
    "args.random_sample = False\n",
    "args.verbose = True\n",
    "args.threads = 1\n",
    "\n",
    "model_fitter = ModelFitter(DefaultModel(), args = args)\n",
    "\n",
    "params, loglik_train, loglik_test = model_fitter.cross_validate(fold_data, fold_number - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df73ec0",
   "metadata": {},
   "source": [
    "We are now ready to begin running our model. We will begin with the default model and then feed it to our  `ModelFitter` class. Note that this code may take a very long time to run (a couple hours)...\n",
    "\n",
    "If you are using multiple threads and seeing a thread-related error, please make sure to turn OFF `%load_ext autoreload` and `%autoreload 2` from the import statements above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea19a9-5155-4b01-b72d-3923b413d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Value, set_start_method\n",
    "from prodict import Prodict\n",
    "\n",
    "# first, we have to check to see if all the splits are there ...\n",
    "assert np.all([f\"{i + 1}.csv\" in os.listdir(data_path) for i in range(n_splits)])\n",
    "print(\"Detected splits in this directory. Loading splits ...\")\n",
    "\n",
    "# then we read them in\n",
    "splits = [pd.read_csv(f\"{data_path}/{i + 1}.csv\") for i in range(n_splits)]\n",
    "\n",
    "# we convert every row of our CSV to a \"CSVMove object\" using df_to_CSVMove - we do so for all the splits\n",
    "# CSVMove is a class that is defined in the parsers.py file \n",
    "fold_data = [[csvmove for csvmove in df_to_CSVMove(split, warn = False)] for split in splits]\n",
    "\n",
    "random.seed(10)\n",
    "initialize_thread_pool(1, manual_seed = 10)\n",
    "\n",
    "args = Prodict()\n",
    "args.random_sample = None\n",
    "args.verbose = True\n",
    "args.threads = 1\n",
    "\n",
    "cross_validate(TreeSearch(), splits, leave_out_idx=fold_number - 1, subsample = 1, threads = args.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_fit import *\n",
    "random.seed(10)\n",
    "initialize_thread_pool(1, manual_seed = 10)\n",
    "\n",
    "model_fitter = ModelFitter(DefaultModel(), \n",
    "                           random_sample = random_sample, \n",
    "                           verbose = verbose, \n",
    "                           threads = threads)\n",
    "\n",
    "model_fitter.fit_model(fold_data[fold_number - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6f368",
   "metadata": {},
   "source": [
    "We can examine the fitted model parameters below ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75555658",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglik_train_df = pd.DataFrame(loglik_train, columns = [\"loglik_train\"])\n",
    "loglik_test_df = pd.DataFrame(loglik_test, columns = [\"loglik_test\"])\n",
    "\n",
    "print(\"Fitted Model Parameters\")\n",
    "param_df = pd.DataFrame(dict(zip(model_fitter.model.param_names, params), index = [0])).drop(\"index\", axis = 1)\n",
    "param_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c963c43",
   "metadata": {},
   "source": [
    "## Part 3: Saving Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae9488",
   "metadata": {},
   "source": [
    "We will save the log likelihood for the train and test folds, as well as the parameters, as `{fold_number}_lltrain.csv`, `{fold_number}_lltest.csv`, and `{fold_number}_params.csv` in the directory specified by `{output_path}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglik_train_df.to_csv(f\"{output_path}/{fold_number}_lltrain.csv\", index = False)\n",
    "loglik_test_df.to_csv(f\"{output_path}/{fold_number}_lltest.csv\", index = False)\n",
    "param_df.to_csv(f\"{output_path}/{fold_number}_params.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d58722",
   "metadata": {},
   "source": [
    "# Running an Existing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add4056",
   "metadata": {},
   "source": [
    "## Loading the Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414ce28",
   "metadata": {},
   "source": [
    "First, let's load our model parameters in from a csv file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_path = f\"{output_path}/{fold_number}_params.csv\"\n",
    "param_df = pd.read_csv(param_path)\n",
    "params = param_df.iloc[0].values\n",
    "\n",
    "# set the parameters of the model that will be used in the tree search\n",
    "model = DefaultModel()\n",
    "heuristic = model.create_heuristic(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b54e53",
   "metadata": {},
   "source": [
    "## Predicting moves on an example board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a5a63",
   "metadata": {},
   "source": [
    "Let's start by taking an arbitrary board state and run our model prediction on it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b74e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "black = 16793616\t\n",
    "white = 12582912\n",
    "show(black, white)\n",
    "\n",
    "# create a fourbynineboard object out of our patterns ...\n",
    "board = fourbynine_board(fourbynine_pattern(black), fourbynine_pattern(white))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = model.create_search(params, heuristic, board)\n",
    "search.complete_search()\n",
    "best_move_index = heuristic.get_best_move(search.get_tree()).board_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4, 3))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "show(black, white, existing_fig=(fig, ax))\n",
    "add_circle(best_move_index, existing_fig=(fig, ax), color = \"blue\")\n",
    "print(\"Predicted Move shown in BLUE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e9771",
   "metadata": {},
   "source": [
    "## Make a Heatmap of Predictions from Multiple Searches\n",
    "You might notice that there is noisiness in the behavior of the model. To show how the model behaves over several runs, we'll want to show a heatmap of the different predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ecfa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "counts = np.zeros(36)\n",
    "for _ in tqdm(range(n_samples), leave=True): \n",
    "    search = model.create_search(params, heuristic, board)\n",
    "    search.complete_search()\n",
    "    best_move_index = heuristic.get_best_move(search.get_tree()).board_position\n",
    "    counts[best_move_index] += 1\n",
    "\n",
    "fig = plt.figure(figsize = (4, 3))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "show(black, white, existing_fig=(fig, ax))\n",
    "\n",
    "# the [::-1] is to flip the board because of a quirk in the way the board is plotted\n",
    "# you basically have to mirror it over the y-axis (see \"extent\" below)\n",
    "ax.imshow(counts.reshape(4, 9)[::-1], cmap = \"Blues\", extent=[85, 715, -160, -440], alpha = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singularity",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
